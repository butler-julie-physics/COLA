
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Notebook 2: What is a Neural Network? &#8212; Jupyter Books as Digital Textbooks</title>
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Notebook 3: Solving Differential Equations with Neural Networks" href="Notebook3.html" />
    <link rel="prev" title="Notebook 1: Solving Differential Equations Numerically" href="Notebook1.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Jupyter Books as Digital Textbooks</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Jupyter Books: An Easy to Use Software for Creating Digital Textbooks
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="NN_for_CM.html">
   Solving Differential Equations in Classical Mechanics with Neural Networks
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="Notebook1.html">
     Notebook 1: Solving Differential Equations Numerically
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Notebook 2: What is a Neural Network?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Notebook3.html">
     Notebook 3: Solving Differential Equations with Neural Networks
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/Notebook2.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/executablebooks/jupyter-book"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2FNotebook2.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/docs/Notebook2.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-a-neural-network">
   What is a neural network?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#neural-network-terminology">
   Neural Network Terminology
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#fully-connected-feedforward-neural-network-fully-connected-ffnn">
   Fully Connected Feedforward Neural Network (Fully Connected FFNN)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#mathematics-of-a-neuron">
   Mathematics of  a Neuron
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#activation-functions">
   Activation Functions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-note-on-hyperparameters">
   A Note on Hyperparameters
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#mathematics-of-neural-networks-the-first-hidden-layer">
   Mathematics of Neural Networks: The First Hidden Layer
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#mathematics-of-neural-networks-the-second-hidden-layer">
   Mathematics of Neural Networks: The Second Hidden Layer
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#mathematics-of-neural-networks-the-l-th-hidden-layer">
   Mathematics of Neural Networks: The l-th Hidden Layer
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#neural-network-loss-function">
   Neural Network Loss Function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#finding-the-optimized-weights-and-biases">
   Finding The Optimized Weights and Biases
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#forward-pass">
   Forward Pass
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#backpropagation">
   Backpropagation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#creating-a-neural-network-from-scratch-using-jax">
   Creating a Neural Network from Scratch Using JAX
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generate-the-data-set">
     Generate the Data Set
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#perform-a-train-test-split">
     Perform a Train-Test Split
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#define-the-neural-network">
     Define the Neural Network
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#define-the-loss-function">
     Define the Loss Function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#train-the-neural-network">
     Train the Neural Network
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#analyze-the-results">
     Analyze the Results
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#neural-networks-with-a-popular-python-library">
   Neural Networks with a Popular Python Library
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#keras">
     Keras
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#practice-what-you-have-learned">
   Practice What You Have Learned
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="notebook-2-what-is-a-neural-network">
<h1>Notebook 2: What is a Neural Network?<a class="headerlink" href="#notebook-2-what-is-a-neural-network" title="Permalink to this headline">¶</a></h1>
<p>View this notebook on <a class="reference external" href="https://colab.research.google.com/drive/1uHGsQ0q4DUQ4Q-rcRDlYH3IeHi3OLGci?usp=sharing">Google Colab</a>.</p>
<p>This notebook will briefly go through the aspects of neural networks that will important for this application.  For a more general overview of neural networks, please see the set of lectures and exercises located <a class="reference external" href="https://github.com/GDS-Education-Community-of-Practice/DSECOP/tree/IntroDeepLearn/IntroDeepLearning/lectures">here</a>.</p>
<div class="section" id="what-is-a-neural-network">
<h2>What is a neural network?<a class="headerlink" href="#what-is-a-neural-network" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Neural networks can be defined as computational systems that can learn to perform tasks by considering examples, generally without being programmed with any task-specific rules. Another way to phrase this is that a neural network is a computational system that learns to match a given input to the correct output. They are a broad category of machine learning algorithms that include popular algorithms such as convolutionnal neural networks, recurrent neural networks, and deep learning.</p></li>
</ul>
</div>
<div class="section" id="neural-network-terminology">
<h2>Neural Network Terminology<a class="headerlink" href="#neural-network-terminology" title="Permalink to this headline">¶</a></h2>
<p><img alt="NN" src="https://raw.githubusercontent.com/GDS-Education-Community-of-Practice/DSECOP/main/Solving_Differential_Equations_with_NNs/nn.png" /></p>
<ul class="simple">
<li><p>Neuron (Node): the simplest unit of a neural network, takes in an input and produces an output.  Neurons are represent by circles in the above diagram.</p></li>
<li><p>Layer: a collection of neurons that act together.  Layers are represented by vertical stacks of neurons in the above diagram.</p></li>
<li><p>Input Layer: the first layer in a neural network, performs no manipulations to the data.  The layer to the very left in the above diagram is the input layer.</p></li>
<li><p>Output Layer: the last layer of a neural network.  The layer to the very left in the above diagram is the output layer.</p></li>
<li><p>Hidden Layer: any other layer of a neural network, in between the input and output layers.  All other layers besides the input and output layers in the above diagram are hidden layers.</p></li>
</ul>
</div>
<div class="section" id="fully-connected-feedforward-neural-network-fully-connected-ffnn">
<h2>Fully Connected Feedforward Neural Network (Fully Connected FFNN)<a class="headerlink" href="#fully-connected-feedforward-neural-network-fully-connected-ffnn" title="Permalink to this headline">¶</a></h2>
<p><img alt="NN" src="https://raw.githubusercontent.com/GDS-Education-Community-of-Practice/DSECOP/main/Solving_Differential_Equations_with_NNs/nn.png" /></p>
<p>Though there are many types of neural network, using just the phrase “neural network” typically refers to a type of neural network known as a fully connected feedforward neural network (FFNN).  This type of network can also be known as a multilayer perceptron (MLP) if it has at least one hidden layer.</p>
<p>Information in an FFNN moves only forward (left to right in the above diagram).  Additionally each neuron is connected to every neuron in the next layer and there are no connection between neurons in the same layer.  This means that the input to a layer in the neural network is simply the output from the previous layer.  As we will see in a moment, each neuron recienves a weighted sum of the outputs of all neurons in the previous layer.</p>
</div>
<div class="section" id="mathematics-of-a-neuron">
<h2>Mathematics of  a Neuron<a class="headerlink" href="#mathematics-of-a-neuron" title="Permalink to this headline">¶</a></h2>
<p>Each neuron is a mathematical function involving a column from a weight matrix, a scalar from a bias vector, and an activation function.  We can represent the mathematical form of the i-th neuron as:</p>
<p><span class="math notranslate nohighlight">\(\hat{y}_i = f(\sum_{j=1}^M w_{ij}x_j + b_i),\)</span></p>
<p>where x is the input to the neural network, w is the weight matrix which scales the input of the neuron, b is the bias vector that makes sure the output of the neuron is non-zero, and f is known as the activation function, which adds nonlinearity to the neuron.</p>
</div>
<div class="section" id="activation-functions">
<h2>Activation Functions<a class="headerlink" href="#activation-functions" title="Permalink to this headline">¶</a></h2>
<p>There are many common function to use as activation functions for neural networks, but three of the most commons ones are listed here.</p>
<p><strong>Sigmoid</strong></p>
<p><span class="math notranslate nohighlight">\(f(x) = \frac{1}{1 + e^{-x}}\)</span></p>
<p><strong>Hyperbolic Tangent</strong></p>
<p><span class="math notranslate nohighlight">\(f(x) = tanh(x) = \frac{e^{2x} - 1}{e^{2x} + 1}\)</span></p>
<p><strong>Relu (Rectified Linear Unit)</strong></p>
<p><span class="math notranslate nohighlight">\(f(x) = max(0, x)\)</span></p>
</div>
<div class="section" id="a-note-on-hyperparameters">
<h2>A Note on Hyperparameters<a class="headerlink" href="#a-note-on-hyperparameters" title="Permalink to this headline">¶</a></h2>
<p>The number of hidden layers, number of neurons per layer, activation function, and many other features are called hyperparameters of a neural network, meaning that their values must be chosen by the user before the network is run. Changing the value of a hyperparameter can drastically change the results of the neural network.</p>
</div>
<div class="section" id="mathematics-of-neural-networks-the-first-hidden-layer">
<h2>Mathematics of Neural Networks: The First Hidden Layer<a class="headerlink" href="#mathematics-of-neural-networks-the-first-hidden-layer" title="Permalink to this headline">¶</a></h2>
<p>For each neuron, i, in the first hidden layer of a neural network, we can represent its mathematical form as:</p>
<p><span class="math notranslate nohighlight">\(\hat{y}_i^1 = f^1(\sum_{j=1}^M w_{ij}^1x_j + b_i^1).\)</span></p>
<p>We can also write out the mathematical form for the entire first hidden layer as:</p>
<p><span class="math notranslate nohighlight">\(\hat{y}_1 = f^1(W_1\textbf{x} + \textbf{b}_1)\)</span></p>
</div>
<div class="section" id="mathematics-of-neural-networks-the-second-hidden-layer">
<h2>Mathematics of Neural Networks: The Second Hidden Layer<a class="headerlink" href="#mathematics-of-neural-networks-the-second-hidden-layer" title="Permalink to this headline">¶</a></h2>
<p>Similarly for the second hidden layer, we can represent the mathematical form of each neuron as:</p>
<p><span class="math notranslate nohighlight">\(y_i^2 = f^2(\sum_{j=1}^N w_{ij}^2y_j^1 + b_i^2).\)</span></p>
<p>Note here that the weights matriz is no longer multiplied by the inputs to the neural network (x), but rather to the output of the first hidden layer.  This is because the input to the first hidden layer is the input ot the neural network but the input to the second hidden layer is the output of the first hidden layer.  Therefore we can expand the above equation to be a bit more clear:</p>
<p><span class="math notranslate nohighlight">\(y_i^2 = f^2(\sum_{j=1}^N w_{ij}^2f^1(\sum_{k=1}^M w_{kj}^1x_k + b_j^1) + b_i^2).\)</span></p>
<p>We can also write a mathematical form for the entire second hidden layer as:</p>
<p><span class="math notranslate nohighlight">\(\hat{y}_2 = f^2(W_2\hat{y}_1 + \textbf{b}_2),\)</span></p>
<p>or more explicity as</p>
<p><span class="math notranslate nohighlight">\(\hat{y}_2 = f^2(W_2f^1(W_1\textbf(x) + \textbf{b}_1) + \textbf{b}_2).\)</span></p>
</div>
<div class="section" id="mathematics-of-neural-networks-the-l-th-hidden-layer">
<h2>Mathematics of Neural Networks: The l-th Hidden Layer<a class="headerlink" href="#mathematics-of-neural-networks-the-l-th-hidden-layer" title="Permalink to this headline">¶</a></h2>
<p>Finally, we can use the pattern we have developed to write down the equation for the mathematical output for a neuron on the l-th hidden layer of the neural network.  For the i-th neuron on the l-th layer we can describe it mathematically as:</p>
<p><span class="math notranslate nohighlight">\(y_i^l = f^l(\sum_{j=1}^{N_l} w_{ij}^ly_j^{l-1} + b_i^l),\)</span></p>
<p>and more explicitly as</p>
<p><span class="math notranslate nohighlight">\(y_i^l = f^l(\sum_{j=1}^{N_l} w_{ij}^lf^{l-1}(\sum_{k=1}^{N_{l-1}} w_{kj}^{l-2}y_k^{l-1} + b_j^{l-1}) + b_i^l),\)</span></p>
<p>and finally all the way expanded as</p>
<p><span class="math notranslate nohighlight">\(y_i^l = f^l(\sum_{j=1}^{N_l} w_{ij}^l f^{l-1}(\sum_{k=1}^{N_{l-1}} w_{jk}^{l-1}( \cdot \cdot \cdot f^1(\sum_{n=1}^M w_{mn}^1x_n + b_m^1) \cdot \cdot \cdot ) + b_k^{l-1}) + b_j^l).\)</span></p>
<p>We can also write a mathematical expression for the output of the entire l-th layer as:</p>
<p><span class="math notranslate nohighlight">\(\hat{y}_l = f^l (W_l \hat{y}_{l-1} + \textbf{b}_l),\)</span></p>
<p>which can be expanded to</p>
<p><span class="math notranslate nohighlight">\(\hat{y}_l = f^l(W_lf^{l-1}(W_{l-1}\hat{y}_{l-2} + \textbf{b}_{l-1}) + \textbf{b}_l),\)</span></p>
<p>and finally to</p>
<p><span class="math notranslate nohighlight">\(\hat{y}_l = f^l(W_lf^{l-1}(W_{l-1}(\cdot \cdot \cdot f^1(W_1\textbf{x} + \textbf{b}_1) \cdot \cdot \cdot) + \textbf{b}_{l-1}) + \textbf{b}_l).\)</span></p>
<p>Its a complicated expression and can grow to be very large but it is a set equation that describes the output of a neural network with l-1 hidden layers and an output layer. So it is also possible to rephrase the defiition of a neural network to be an analytical function that maps a set of inputs to a set of outputs using a set of optimized parameters.</p>
</div>
<div class="section" id="neural-network-loss-function">
<h2>Neural Network Loss Function<a class="headerlink" href="#neural-network-loss-function" title="Permalink to this headline">¶</a></h2>
<p>A loss function is used to determine how much the output from a neural network differs from the true/expected result.  There is not a set loss function that is used with neural networks, but two common loss functions are the mean-squared error loss function and the mean absolute error function.  The mean-squared error loss function (MSE) can be defined as:</p>
<p><span class="math notranslate nohighlight">\(J_{MSE}(W) = \frac{1}{N}\sum_{i=1}^N (y_i - \hat{y}_i)^2,\)</span></p>
<p>where y is the true data set, <span class="math notranslate nohighlight">\(\hat{y}\)</span> is the neural network prediction, N is the number of data points in the set, and W are the weights of the neural network.  The loss function depends on the weights of the neural network because changing the weights of the neural network changes its output.</p>
<p>The mean-absolute error loss function (MAE) has a similar form:</p>
<p><span class="math notranslate nohighlight">\(J_{MAE}(W) = \frac{1}{N}\sum_{i=1}^N |y_i - \hat{y}_i|.\)</span></p>
</div>
<div class="section" id="finding-the-optimized-weights-and-biases">
<h2>Finding The Optimized Weights and Biases<a class="headerlink" href="#finding-the-optimized-weights-and-biases" title="Permalink to this headline">¶</a></h2>
<p>A major part of working with neural networks is a process known as training where the weights of the neural network are optimized such that the cost function is minimized.  This training process has two phases: the forward pass and the backpropagation.</p>
</div>
<div class="section" id="forward-pass">
<h2>Forward Pass<a class="headerlink" href="#forward-pass" title="Permalink to this headline">¶</a></h2>
<p><img alt="NN" src="https://raw.githubusercontent.com/GDS-Education-Community-of-Practice/DSECOP/main/Solving_Differential_Equations_with_NNs/bp1.png" /></p>
<p>The forward pass occurs when data is sent through the neural network (from left to right on the above graph) to produce a predicted output.  This predicted output is then fed into the loss function with the true data set to generate the loss value.</p>
</div>
<div class="section" id="backpropagation">
<h2>Backpropagation<a class="headerlink" href="#backpropagation" title="Permalink to this headline">¶</a></h2>
<p><img alt="NN" src="https://raw.githubusercontent.com/GDS-Education-Community-of-Practice/DSECOP/main/Solving_Differential_Equations_with_NNs/bp2.png" /></p>
<p>After the forward pass comes backpropagation, where the error from the loss function is backpropagated through the layers of the neural networks and its weights are adjusted layer by layer so that the next forward pass will result in a reduced loss value.  A simple way to optimize the weights of a neural network during backpropagation is through an optimization technique known as gradient descent.  The weights of the neural network are simply adjusted by the derivative of the loss function with respect to the weights, scaled by a hyperparameter known as the learning rate:</p>
<p><span class="math notranslate nohighlight">\(W = W - r_{l}\frac{\partial J(W}{\partial W}.\)</span></p>
<p>The learning rate (r<span class="math notranslate nohighlight">\(_l\)</span>) is a number typically much less than 1 and it is also a hyperparameter, so its value must be set before the neural network is run.</p>
<p>The process of training a neural network involves many different iterations of forward pass followed by backpropagation.  Typically a training process will continue until a certain number of training iterations has been reached or the difference in the current loss value compared to the value from the previous iteration is below a certain threshold.  However, neural networks should not be trained for an overly long time because this will lead to something called overfitting where the neural network learns to match the data set it is trained with very well (so will show a small loss value) but loses all generality when given new data (so it will perform poorly when given the new data set).</p>
<p><strong>EXERCISE 1</strong>: Take a moment and summarize what you have learned about neural networks in the text box below.</p>
<p>Delete this text and type your response here.</p>
</div>
<div class="section" id="creating-a-neural-network-from-scratch-using-jax">
<h2>Creating a Neural Network from Scratch Using JAX<a class="headerlink" href="#creating-a-neural-network-from-scratch-using-jax" title="Permalink to this headline">¶</a></h2>
<p>JAX is an automatic differentiation library in Python that can find the derivative of any chunk of code it is given.  If you are interested you can read more about the library <a class="reference external" href="https://github.com/google/jax">here</a>.</p>
<p>The below section of the notebook will create a neural network entirely from scratch and analyze its performance. We will be using the gradient feature of the JAX library to implement a gradient descent optimization of our loss function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># IMPORTS</span>
<span class="c1"># Math for the ceiling function</span>
<span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">ceil</span>
<span class="c1"># Matplotlib for graphing capabilities</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="c1"># Numpy for arrays</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="c1"># Modules from the JAX library for creating neural networks</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">grad</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">random</span> <span class="k">as</span> <span class="n">npr</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="o">/</span><span class="n">var</span><span class="o">/</span><span class="n">folders</span><span class="o">/</span><span class="n">nn</span><span class="o">/</span><span class="n">x8vw0d756190nyy38jxd8nlh0000gn</span><span class="o">/</span><span class="n">T</span><span class="o">/</span><span class="n">ipykernel_11060</span><span class="o">/</span><span class="mf">3931833151.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">      </span><span class="mi">7</span> <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="g g-Whitespace">      </span><span class="mi">8</span> <span class="c1"># Modules from the JAX library for creating neural networks</span>
<span class="ne">----&gt; </span><span class="mi">9</span> <span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
<span class="g g-Whitespace">     </span><span class="mi">10</span> <span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">grad</span>
<span class="g g-Whitespace">     </span><span class="mi">11</span> <span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">random</span> <span class="k">as</span> <span class="n">npr</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;jax&#39;
</pre></div>
</div>
</div>
</div>
<div class="section" id="generate-the-data-set">
<h3>Generate the Data Set<a class="headerlink" href="#generate-the-data-set" title="Permalink to this headline">¶</a></h3>
<p>Let’s keep things simple and generate a data points from a Gaussian curve.  We will have our x data be evenly space between -10 and 10 and our y data be the corresponding points on a Gaussian curve.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Let&#39;s create a data set that is just a basic Gaussian curve</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">250</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">X</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="perform-a-train-test-split">
<h3>Perform a Train-Test Split<a class="headerlink" href="#perform-a-train-test-split" title="Permalink to this headline">¶</a></h3>
<p>In machine learning problems it is common to split your data into two sets.  The first set, which usually contains 70%-80% of the data, is called the training set.  This set of data is used to train the machine learning algorithm (i.e. set the weights such that the cost function is minimized).  The second data set, which is much smaller, is called the test set.  This is used to test the accuracy of the machine learning algorithm on data that it has not yet seen.</p>
<p>Below we use the common training-test split functionality from the library Scikit-Learn.  We will be using 80% of our total data set as the training set with the remaining 20% being the test set.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># We will split the data set into two pieces, a training data set that contains</span>
<span class="c1"># 80% of the total data and a test set that contains the other 20%</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">train_size</span> <span class="o">=</span> <span class="mf">0.8</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="n">train_size</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="define-the-neural-network">
<h3>Define the Neural Network<a class="headerlink" href="#define-the-neural-network" title="Permalink to this headline">¶</a></h3>
<p>Now we will define a neural network using the equations for neural networks defined above.  First we will define the sigmoid function as our activation function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Calculates the value of the sigmoid function for </span>
<span class="sd">        a given input of x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="mf">1.</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">+</span> <span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Now we will define our neural network.  Here we will be using an architecture with two hidden layers, each using the sigmoid activation function, and an output layer which does not have an activation function.  Note how the input to the second hidden layer is the output from the first hidden layer and the input to the output layer is the output from the second hidden layer.  We will be setting the number of neurons per layer later when we are training the neural network.  The code we have defined below does not require a specific number of neurons per hidden layer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">neural_network</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Inputs:</span>
<span class="sd">            W (a list of length 2): the weights of the neural network</span>
<span class="sd">            x (a float): the input value of the neural network</span>
<span class="sd">        Returns:</span>
<span class="sd">            Unnamed (a float): The output of the neural network</span>
<span class="sd">        Defines a neural network with one hidden layer.  The number of neurons in</span>
<span class="sd">        the hidden layer is the length of W[0]. The activation function is the </span>
<span class="sd">        sigmoid function on the hidden layer an none on the output layer.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Calculate the output for the neurons in the hidden layers</span>
    <span class="n">hidden_layer1</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">W</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
    <span class="n">hidden_layer2</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">hidden_layer1</span><span class="p">,</span> <span class="n">W</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
    <span class="c1"># Calculate the result for the output neuron</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">hidden_layer2</span><span class="p">,</span> <span class="n">W</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="define-the-loss-function">
<h3>Define the Loss Function<a class="headerlink" href="#define-the-loss-function" title="Permalink to this headline">¶</a></h3>
<p>Now we need to define our loss function.  For simplicity we will be using the mean-squared error loss function, which is a very common loss function for training neural networks.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">loss_function</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Inputs:</span>
<span class="sd">            W (a list): the weights of the neural network</span>
<span class="sd">            t (a 1D NumPy array): the times to calculate the predicted position at</span>
<span class="sd">        Returns:</span>
<span class="sd">            loss_sum (a float): The total loss over all times</span>
<span class="sd">        The loss function for the neural network to solve for position given </span>
<span class="sd">        a function for acceleration.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Define a variable to hold the total loss</span>
    <span class="n">loss_sum</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="c1"># Loop through each individual time</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
        <span class="c1"># Get the output of the neural network with the given set of weights</span>
        <span class="n">nn</span> <span class="o">=</span> <span class="n">neural_network</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">])[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">err_sqr</span> <span class="o">=</span> <span class="p">(</span><span class="n">nn</span><span class="o">-</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span>
        <span class="c1"># Update the loss sum</span>
        <span class="n">loss_sum</span> <span class="o">+=</span> <span class="n">err_sqr</span>
    <span class="n">loss_sum</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="c1"># Return the loss sum    </span>
    <span class="k">return</span> <span class="n">loss_sum</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="train-the-neural-network">
<h3>Train the Neural Network<a class="headerlink" href="#train-the-neural-network" title="Permalink to this headline">¶</a></h3>
<p>Finally we need to train our neural network.  We will start by randomly initializing the weights of our neural network (with 25 neurons per hidden layer).  We then define the parameters for the learning rate, the number of training iterations, and the threshold for stopping the training.  Next, we perform gradient descent to update the weights of the neural network over for the set number of training iterations, or until the loss function value converges to some set threshold.</p>
<p><strong>WARNING</strong>: This cell will take a long time to run.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Generate the key for the random number generator</span>
<span class="n">key</span> <span class="o">=</span> <span class="n">npr</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># Set the number of neurons in the hidden layer</span>
<span class="n">number_hidden_neurons</span> <span class="o">=</span> <span class="mi">25</span>
<span class="c1"># Initialize the weights of the neural network with random numbers</span>
<span class="n">W</span> <span class="o">=</span> <span class="p">[</span><span class="n">npr</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">key</span><span class="p">,(</span><span class="mi">1</span><span class="p">,</span> <span class="n">number_hidden_neurons</span><span class="p">)),</span> 
     <span class="n">npr</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">key</span><span class="p">,(</span><span class="n">number_hidden_neurons</span><span class="p">,</span><span class="n">number_hidden_neurons</span><span class="p">)),</span> 
     <span class="n">npr</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">key</span><span class="p">,(</span><span class="n">number_hidden_neurons</span><span class="p">,</span> <span class="mi">1</span><span class="p">))]</span>

<span class="c1"># Set the learning rate and the number of training iterations for the network</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">num_training_iterations</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">threshold</span> <span class="o">=</span> <span class="mf">0.0001</span>
<span class="n">previous_loss</span> <span class="o">=</span> <span class="mi">0</span>

<span class="c1"># Train the neural network for the specified number of iterations</span>
<span class="c1"># Update the weights using the learning rates</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_training_iterations</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training Iteration:&quot;</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">current_loss</span> <span class="o">=</span> <span class="n">loss_function</span><span class="p">(</span><span class="n">W</span><span class="p">,</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Loss:&quot;</span><span class="p">,</span> <span class="n">current_loss</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="c1"># If the current loss is within a set threshold of the previous loss, stop</span>
    <span class="c1"># the training</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">current_loss</span><span class="o">-</span><span class="n">previous_loss</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">threshold</span><span class="p">:</span>
        <span class="k">break</span><span class="p">;</span>
    <span class="c1"># Calculate the gradient of the loss function and then use that gradient to</span>
    <span class="c1"># update the weights of the neural network using the learning rate and the </span>
    <span class="c1"># gradient descent optimization method</span>
    <span class="n">loss_grad</span> <span class="o">=</span>  <span class="n">grad</span><span class="p">(</span><span class="n">loss_function</span><span class="p">)(</span><span class="n">W</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">W</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">W</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">loss_grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">W</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">W</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">loss_grad</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">W</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">W</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">loss_grad</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">previous_loss</span> <span class="o">=</span> <span class="n">current_loss</span>
    
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training Iteration: 1
Loss: 0.9679325

Training Iteration: 2
Loss: 0.87496966

Training Iteration: 3
Loss: 0.42899132

Training Iteration: 4
Loss: 0.56665987

Training Iteration: 5
Loss: 0.28957328

Training Iteration: 6
Loss: 0.37441632

Training Iteration: 7
Loss: 0.38987857

Training Iteration: 8
Loss: 0.40654492

Training Iteration: 9
Loss: 0.2975046

Training Iteration: 10
Loss: 0.33554095

Training Iteration: 11
Loss: 0.28734046

Training Iteration: 12
Loss: 0.31077173

Training Iteration: 13
Loss: 0.2668793

Training Iteration: 14
Loss: 0.28293598

Training Iteration: 15
Loss: 0.25128502

Training Iteration: 16
Loss: 0.26022694

Training Iteration: 17
Loss: 0.23660965

Training Iteration: 18
Loss: 0.2390252

Training Iteration: 19
Loss: 0.22199869

Training Iteration: 20
Loss: 0.21908057

Training Iteration: 21
Loss: 0.20718534

Training Iteration: 22
Loss: 0.20055237

Training Iteration: 23
Loss: 0.19233082

Training Iteration: 24
Loss: 0.18365496

Training Iteration: 25
Loss: 0.17786014

Training Iteration: 26
Loss: 0.16854252

Training Iteration: 27
Loss: 0.1642623

Training Iteration: 28
Loss: 0.15527935

Training Iteration: 29
Loss: 0.1519319

Training Iteration: 30
Loss: 0.1438376

Training Iteration: 31
Loss: 0.1410901

Training Iteration: 32
Loss: 0.13411222

Training Iteration: 33
Loss: 0.13178869

Training Iteration: 34
Loss: 0.1259442

Training Iteration: 35
Loss: 0.12395257

Training Iteration: 36
Loss: 0.119144745

Training Iteration: 37
Loss: 0.11743273

Training Iteration: 38
Loss: 0.113516614

Training Iteration: 39
Loss: 0.11204696

Training Iteration: 40
Loss: 0.10886968

Training Iteration: 41
Loss: 0.1076121

Training Iteration: 42
Loss: 0.10503288

Training Iteration: 43
Loss: 0.103959225

Training Iteration: 44
Loss: 0.101857044

Training Iteration: 45
Loss: 0.1009418

Training Iteration: 46
Loss: 0.09921744

Training Iteration: 47
Loss: 0.09843698

Training Iteration: 48
Loss: 0.097010955

Training Iteration: 49
Loss: 0.09634497

Training Iteration: 50
Loss: 0.09515467

Training Iteration: 51
Loss: 0.09458538

Training Iteration: 52
Loss: 0.09358216

Training Iteration: 53
Loss: 0.09309526

Training Iteration: 54
Loss: 0.09224184

Training Iteration: 55
Loss: 0.09182528

Training Iteration: 56
Loss: 0.091092415

Training Iteration: 57
Loss: 0.09073689

Training Iteration: 58
Loss: 0.09010229

Training Iteration: 59
Loss: 0.08980025

Training Iteration: 60
Loss: 0.08924679

Training Iteration: 61
Loss: 0.08899272

Training Iteration: 62
Loss: 0.08850703

Training Iteration: 63
Loss: 0.088296615

Training Iteration: 64
Loss: 0.087868385

Training Iteration: 65
Loss: 0.08769861

Training Iteration: 66
Loss: 0.08731979

Training Iteration: 67
Loss: 0.08718859

Training Iteration: 68
Loss: 0.08685253

Training Iteration: 69
Loss: 0.086758174
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="analyze-the-results">
<h3>Analyze the Results<a class="headerlink" href="#analyze-the-results" title="Permalink to this headline">¶</a></h3>
<p>Now we need to analyze the performance of our neural network using the test data set that was reserved earlier.  First we need to generate the neural network predictions for the y component of the test data set.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_nn</span> <span class="o">=</span> <span class="p">[</span><span class="n">neural_network</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">xi</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">xi</span> <span class="ow">in</span> <span class="n">X_test</span><span class="p">]</span> 
</pre></div>
</div>
</div>
</div>
<p>First lets analyze the results graphically by plotting the predicted test data set and the true test data set on the same graph.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_nn</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;NN Prediction&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;True Data&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;X&quot;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;y&#39;)
</pre></div>
</div>
<img alt="_images/Notebook2_33_1.png" src="_images/Notebook2_33_1.png" />
</div>
</div>
<p>Next let’s analyze the error numerically using the root mean-squared error (RMSE) function, which is simiply the square root of the mean-squared error.  The RMSE gives the average error on each data point (instead of the squared average error) so it is a met more of a clear metric for error analysis.  First, let’s define a function to calculate the RMSE between two data sets.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">rmse</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">B</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Inputs:</span>
<span class="sd">            A,B (NumPy arrays)</span>
<span class="sd">        Returns:</span>
<span class="sd">            Unnamed (a float): the RMSE error between A and B</span>
<span class="sd">        Calculates the RMSE error between A and B.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">A</span><span class="p">)</span><span class="o">==</span><span class="nb">len</span><span class="p">(</span><span class="n">B</span><span class="p">),</span><span class="s2">&quot;The data sets must be the same length to calcualte</span><span class="se">\</span>
<span class="s2">        the RMSE.&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">((</span><span class="n">A</span><span class="o">-</span><span class="n">B</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span> 
</pre></div>
</div>
</div>
</div>
<p>Now let’s print the RMSE between the true test data set and the neural network prediction.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;RMSE between true test set and neural network result:&quot;</span><span class="p">,</span> <span class="n">rmse</span><span class="p">(</span><span class="n">y_nn</span><span class="p">,</span><span class="n">y_test</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>RMSE between true test set and neural network result: 0.4083000246981841
</pre></div>
</div>
</div>
</div>
<p><strong>EXERCISE 2</strong>: Are you satisfied with the performance of this neural network?  If not, brainstorm some ways in which its performance may be improved.</p>
<p>Delete this text and type your response here.</p>
</div>
</div>
<div class="section" id="neural-networks-with-a-popular-python-library">
<h2>Neural Networks with a Popular Python Library<a class="headerlink" href="#neural-networks-with-a-popular-python-library" title="Permalink to this headline">¶</a></h2>
<p>When using a neural network for most cases, instead of creating one by hand you will likely use a neural network implementation from a popular Python library.  These have several advantages including being able to easily use large networks (which would be hard to create by hand), the use of more advanced optimizers for training, and more optimized implementations to significantly decrease runtime (and likely more accuracy).</p>
<div class="section" id="keras">
<h3>Keras<a class="headerlink" href="#keras" title="Permalink to this headline">¶</a></h3>
<p>One of the most popular machine learning library in Python is known as Keras, which is actually a wrapper for another machine learning library known as Tensorflow.  See the <a class="reference external" href="https://keras.io/">Keras website</a> for more information.   Keras is very commonly used for training neural networks becayse it makes building and training nerual networks simply and intuitive.</p>
<p>Neural networks are build in keras by adding layers with the specified parameters to a model and then compiling the model with a loss function and an optimizer.  Here we use the same architecture as we have been using (two hidden layers with 25 neurons each and a hyperbolic tangent activation function).  The main difference here is that instead of using the gradient descent algorithm the optimize the weights of the neural network we will instead be using a much more powerful optimizer known as the Adam optimizer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">keras</span>

<span class="c1"># Create the model </span>
<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span> <span class="o">=</span> <span class="mi">25</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;tanh&#39;</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span> <span class="o">=</span> <span class="mi">25</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;tanh&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;tanh&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;mse&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s2">&quot;adam&quot;</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">epochs</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="n">y_nn</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rmse</span><span class="p">(</span><span class="n">y_nn</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_nn</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/200
7/7 [==============================] - 0s 3ms/step - loss: 0.1078
Epoch 2/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0645
Epoch 3/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0427
Epoch 4/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0409
Epoch 5/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0352
Epoch 6/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0326
Epoch 7/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0304
Epoch 8/200
7/7 [==============================] - 0s 4ms/step - loss: 0.0283
Epoch 9/200
7/7 [==============================] - 0s 2ms/step - loss: 0.0264
Epoch 10/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0249
Epoch 11/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0230
Epoch 12/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0216
Epoch 13/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0204
Epoch 14/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0193
Epoch 15/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0182
Epoch 16/200
7/7 [==============================] - 0s 4ms/step - loss: 0.0171
Epoch 17/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0165
Epoch 18/200
7/7 [==============================] - 0s 2ms/step - loss: 0.0155
Epoch 19/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0151
Epoch 20/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0142
Epoch 21/200
7/7 [==============================] - 0s 4ms/step - loss: 0.0134
Epoch 22/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0127
Epoch 23/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0122
Epoch 24/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0118
Epoch 25/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0112
Epoch 26/200
7/7 [==============================] - 0s 4ms/step - loss: 0.0108
Epoch 27/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0103
Epoch 28/200
7/7 [==============================] - 0s 2ms/step - loss: 0.0100
Epoch 29/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0096
Epoch 30/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0093
Epoch 31/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0090
Epoch 32/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0085
Epoch 33/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0082
Epoch 34/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0079
Epoch 35/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0076
Epoch 36/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0074
Epoch 37/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0071
Epoch 38/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0068
Epoch 39/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0066
Epoch 40/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0063
Epoch 41/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0061
Epoch 42/200
7/7 [==============================] - 0s 4ms/step - loss: 0.0059
Epoch 43/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0057
Epoch 44/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0054
Epoch 45/200
7/7 [==============================] - 0s 4ms/step - loss: 0.0052
Epoch 46/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0050
Epoch 47/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0048
Epoch 48/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0046
Epoch 49/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0045
Epoch 50/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0043
Epoch 51/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0041
Epoch 52/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0040
Epoch 53/200
7/7 [==============================] - 0s 2ms/step - loss: 0.0039
Epoch 54/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0037
Epoch 55/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0036
Epoch 56/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0035
Epoch 57/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0034
Epoch 58/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0033
Epoch 59/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0032
Epoch 60/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0030
Epoch 61/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0030
Epoch 62/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0029
Epoch 63/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0028
Epoch 64/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0027
Epoch 65/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0026
Epoch 66/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0026
Epoch 67/200
7/7 [==============================] - 0s 4ms/step - loss: 0.0025
Epoch 68/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0024
Epoch 69/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0024
Epoch 70/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0023
Epoch 71/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0023
Epoch 72/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0022
Epoch 73/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0021
Epoch 74/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0020
Epoch 75/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0020
Epoch 76/200
7/7 [==============================] - 0s 4ms/step - loss: 0.0020
Epoch 77/200
7/7 [==============================] - 0s 5ms/step - loss: 0.0019
Epoch 78/200
7/7 [==============================] - 0s 4ms/step - loss: 0.0019
Epoch 79/200
7/7 [==============================] - 0s 4ms/step - loss: 0.0018
Epoch 80/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0018
Epoch 81/200
7/7 [==============================] - 0s 4ms/step - loss: 0.0018
Epoch 82/200
7/7 [==============================] - 0s 4ms/step - loss: 0.0017
Epoch 83/200
7/7 [==============================] - 0s 4ms/step - loss: 0.0017
Epoch 84/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0016
Epoch 85/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0016
Epoch 86/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0016
Epoch 87/200
7/7 [==============================] - 0s 4ms/step - loss: 0.0016
Epoch 88/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0015
Epoch 89/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0015
Epoch 90/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0015
Epoch 91/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0014
Epoch 92/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0014
Epoch 93/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0013
Epoch 94/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0013
Epoch 95/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0013
Epoch 96/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0013
Epoch 97/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0012
Epoch 98/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0012
Epoch 99/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0012
Epoch 100/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0012
Epoch 101/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0012
Epoch 102/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0011
Epoch 103/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0012
Epoch 104/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0011
Epoch 105/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0011
Epoch 106/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0011
Epoch 107/200
7/7 [==============================] - 0s 3ms/step - loss: 0.0010
Epoch 108/200
7/7 [==============================] - 0s 4ms/step - loss: 0.0010
Epoch 109/200
7/7 [==============================] - 0s 3ms/step - loss: 9.8343e-04
Epoch 110/200
7/7 [==============================] - 0s 3ms/step - loss: 9.9275e-04
Epoch 111/200
7/7 [==============================] - 0s 3ms/step - loss: 9.7353e-04
Epoch 112/200
7/7 [==============================] - 0s 4ms/step - loss: 9.6189e-04
Epoch 113/200
7/7 [==============================] - 0s 3ms/step - loss: 9.3219e-04
Epoch 114/200
7/7 [==============================] - 0s 3ms/step - loss: 9.3055e-04
Epoch 115/200
7/7 [==============================] - 0s 3ms/step - loss: 9.2136e-04
Epoch 116/200
7/7 [==============================] - 0s 4ms/step - loss: 8.8719e-04
Epoch 117/200
7/7 [==============================] - 0s 3ms/step - loss: 8.5708e-04
Epoch 118/200
7/7 [==============================] - 0s 3ms/step - loss: 8.6396e-04
Epoch 119/200
7/7 [==============================] - 0s 4ms/step - loss: 8.2447e-04
Epoch 120/200
7/7 [==============================] - 0s 3ms/step - loss: 8.3418e-04
Epoch 121/200
7/7 [==============================] - 0s 2ms/step - loss: 8.1305e-04
Epoch 122/200
7/7 [==============================] - 0s 3ms/step - loss: 7.7774e-04
Epoch 123/200
7/7 [==============================] - 0s 3ms/step - loss: 7.7009e-04
Epoch 124/200
7/7 [==============================] - 0s 3ms/step - loss: 7.4960e-04
Epoch 125/200
7/7 [==============================] - 0s 3ms/step - loss: 7.4154e-04
Epoch 126/200
7/7 [==============================] - 0s 3ms/step - loss: 7.5855e-04
Epoch 127/200
7/7 [==============================] - 0s 3ms/step - loss: 7.1417e-04
Epoch 128/200
7/7 [==============================] - 0s 3ms/step - loss: 7.0284e-04
Epoch 129/200
7/7 [==============================] - 0s 3ms/step - loss: 7.0224e-04
Epoch 130/200
7/7 [==============================] - 0s 3ms/step - loss: 6.7733e-04
Epoch 131/200
7/7 [==============================] - 0s 4ms/step - loss: 6.6874e-04
Epoch 132/200
7/7 [==============================] - 0s 3ms/step - loss: 6.6444e-04
Epoch 133/200
7/7 [==============================] - 0s 3ms/step - loss: 6.5241e-04
Epoch 134/200
7/7 [==============================] - 0s 3ms/step - loss: 6.4083e-04
Epoch 135/200
7/7 [==============================] - 0s 3ms/step - loss: 6.3749e-04
Epoch 136/200
7/7 [==============================] - 0s 3ms/step - loss: 6.6859e-04
Epoch 137/200
7/7 [==============================] - 0s 3ms/step - loss: 6.3209e-04
Epoch 138/200
7/7 [==============================] - 0s 3ms/step - loss: 6.0695e-04
Epoch 139/200
7/7 [==============================] - 0s 3ms/step - loss: 6.0195e-04
Epoch 140/200
7/7 [==============================] - 0s 4ms/step - loss: 5.9255e-04
Epoch 141/200
7/7 [==============================] - 0s 4ms/step - loss: 5.8917e-04
Epoch 142/200
7/7 [==============================] - 0s 3ms/step - loss: 5.8393e-04
Epoch 143/200
7/7 [==============================] - 0s 3ms/step - loss: 5.6860e-04
Epoch 144/200
7/7 [==============================] - 0s 3ms/step - loss: 5.6240e-04
Epoch 145/200
7/7 [==============================] - 0s 3ms/step - loss: 5.7392e-04
Epoch 146/200
7/7 [==============================] - 0s 4ms/step - loss: 5.7676e-04
Epoch 147/200
7/7 [==============================] - 0s 4ms/step - loss: 5.7071e-04
Epoch 148/200
7/7 [==============================] - 0s 4ms/step - loss: 5.3583e-04
Epoch 149/200
7/7 [==============================] - 0s 4ms/step - loss: 5.2616e-04
Epoch 150/200
7/7 [==============================] - 0s 4ms/step - loss: 5.2702e-04
Epoch 151/200
7/7 [==============================] - 0s 3ms/step - loss: 5.3313e-04
Epoch 152/200
7/7 [==============================] - 0s 3ms/step - loss: 5.0830e-04
Epoch 153/200
7/7 [==============================] - 0s 3ms/step - loss: 5.0240e-04
Epoch 154/200
7/7 [==============================] - 0s 3ms/step - loss: 5.2469e-04
Epoch 155/200
7/7 [==============================] - 0s 3ms/step - loss: 4.9143e-04
Epoch 156/200
7/7 [==============================] - 0s 3ms/step - loss: 4.8827e-04
Epoch 157/200
7/7 [==============================] - 0s 3ms/step - loss: 4.7646e-04
Epoch 158/200
7/7 [==============================] - 0s 3ms/step - loss: 4.6802e-04
Epoch 159/200
7/7 [==============================] - 0s 3ms/step - loss: 4.6930e-04
Epoch 160/200
7/7 [==============================] - 0s 3ms/step - loss: 4.8631e-04
Epoch 161/200
7/7 [==============================] - 0s 4ms/step - loss: 4.5814e-04
Epoch 162/200
7/7 [==============================] - 0s 4ms/step - loss: 4.6103e-04
Epoch 163/200
7/7 [==============================] - 0s 3ms/step - loss: 4.6353e-04
Epoch 164/200
7/7 [==============================] - 0s 3ms/step - loss: 4.6680e-04
Epoch 165/200
7/7 [==============================] - 0s 4ms/step - loss: 4.5697e-04
Epoch 166/200
7/7 [==============================] - 0s 3ms/step - loss: 4.6745e-04
Epoch 167/200
7/7 [==============================] - 0s 3ms/step - loss: 4.3146e-04
Epoch 168/200
7/7 [==============================] - 0s 3ms/step - loss: 4.2322e-04
Epoch 169/200
7/7 [==============================] - 0s 4ms/step - loss: 4.3428e-04
Epoch 170/200
7/7 [==============================] - 0s 3ms/step - loss: 4.1617e-04
Epoch 171/200
7/7 [==============================] - 0s 3ms/step - loss: 4.1878e-04
Epoch 172/200
7/7 [==============================] - 0s 4ms/step - loss: 4.0123e-04
Epoch 173/200
7/7 [==============================] - 0s 3ms/step - loss: 4.0234e-04
Epoch 174/200
7/7 [==============================] - 0s 3ms/step - loss: 3.9102e-04
Epoch 175/200
7/7 [==============================] - 0s 3ms/step - loss: 3.8357e-04
Epoch 176/200
7/7 [==============================] - 0s 3ms/step - loss: 3.7910e-04
Epoch 177/200
7/7 [==============================] - 0s 3ms/step - loss: 3.7660e-04
Epoch 178/200
7/7 [==============================] - 0s 4ms/step - loss: 3.6767e-04
Epoch 179/200
7/7 [==============================] - 0s 3ms/step - loss: 3.6114e-04
Epoch 180/200
7/7 [==============================] - 0s 3ms/step - loss: 3.6740e-04
Epoch 181/200
7/7 [==============================] - 0s 2ms/step - loss: 3.7303e-04
Epoch 182/200
7/7 [==============================] - 0s 3ms/step - loss: 3.5549e-04
Epoch 183/200
7/7 [==============================] - 0s 3ms/step - loss: 3.4674e-04
Epoch 184/200
7/7 [==============================] - 0s 4ms/step - loss: 3.4938e-04
Epoch 185/200
7/7 [==============================] - 0s 4ms/step - loss: 3.4346e-04
Epoch 186/200
7/7 [==============================] - 0s 3ms/step - loss: 3.3951e-04
Epoch 187/200
7/7 [==============================] - 0s 3ms/step - loss: 3.3777e-04
Epoch 188/200
7/7 [==============================] - 0s 3ms/step - loss: 3.3488e-04
Epoch 189/200
7/7 [==============================] - 0s 4ms/step - loss: 3.3452e-04
Epoch 190/200
7/7 [==============================] - 0s 3ms/step - loss: 3.2418e-04
Epoch 191/200
7/7 [==============================] - 0s 3ms/step - loss: 3.2094e-04
Epoch 192/200
7/7 [==============================] - 0s 3ms/step - loss: 3.2427e-04
Epoch 193/200
7/7 [==============================] - 0s 5ms/step - loss: 3.2259e-04
Epoch 194/200
7/7 [==============================] - 0s 3ms/step - loss: 3.1608e-04
Epoch 195/200
7/7 [==============================] - 0s 3ms/step - loss: 3.1022e-04
Epoch 196/200
7/7 [==============================] - 0s 3ms/step - loss: 3.1024e-04
Epoch 197/200
7/7 [==============================] - 0s 3ms/step - loss: 3.1451e-04
Epoch 198/200
7/7 [==============================] - 0s 3ms/step - loss: 3.0206e-04
Epoch 199/200
7/7 [==============================] - 0s 3ms/step - loss: 2.9827e-04
Epoch 200/200
7/7 [==============================] - 0s 3ms/step - loss: 2.9468e-04
0.38217810638619504
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.collections.PathCollection at 0x7f5e7cc28410&gt;
</pre></div>
</div>
<img alt="_images/Notebook2_42_2.png" src="_images/Notebook2_42_2.png" />
</div>
</div>
<p><strong>EXERCISE 3</strong>: Of the three implementations of neural neworks explored here (from scratch, Scikit-Learn, and Keras), which implementation do you think is best?  Consider not just the accuracy of the neural network but also its runtime and ease of use.</p>
<p>Delete this text and type your response here.</p>
</div>
</div>
<div class="section" id="practice-what-you-have-learned">
<h2>Practice What You Have Learned<a class="headerlink" href="#practice-what-you-have-learned" title="Permalink to this headline">¶</a></h2>
<p>Go back to Exercise 2 where you brainstormed ways to improve the performance of the neural network.  Choose three of these ideas that you think are the most promising and implement them one at a time below the cell.  Copy and paste as much code from above as needed.  Record how your changes effect the error when predicting the test data set.  Were you able to get the error significantly lower than the notebook’s result?</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="Notebook1.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Notebook 1: Solving Differential Equations Numerically</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="Notebook3.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Notebook 3: Solving Differential Equations with Neural Networks</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Julie Butler<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>